{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取り除くID: ['MR0011_0742', 'MR0011_0770', 'MR0011_0810', 'MR0011_0681', 'MR0011_0787', 'MR0011_0793', 'MR0011_0761']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# a program to make .fsgd file for fREEsURFER-script fs_glm.sh from .xlsx files\n",
    "#\n",
    "import pandas as pd\n",
    "import os\n",
    "IMPACT_CWD=os.path.expanduser(\"~\")+\"/Downloads/impact/\" # working directory\n",
    "#print(IMPACT_CWD)\n",
    "bl_book = pd.ExcelFile(IMPACT_CWD+'BHQ_Research_2018_01_Access用_20200108.xlsx') # base lien file\n",
    "fu_book = pd.ExcelFile(IMPACT_CWD+'BHQ_Research_2018_02_Access用_20200108.xlsx') # follow up file\n",
    "\n",
    "# 手動で取り除くIDを指定\n",
    "IDsQCfailed=[\"MR0011_0742\",\"MR0011_0770\",\"MR0011_0810\"] # QCでNGのID\n",
    "IDsQCfailed=IDsQCfailed+[\"MR0011_0681\",\"MR0011_0787\",\"MR0011_0793\"] # niftiファイルの無いID\n",
    "IDsQCfailed=IDsQCfailed+[\"MR0011_0761\"] # follow up画像の無いID\n",
    "print(\"取り除くID:\",IDsQCfailed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read base line\n",
    "bl_sheet_name = bl_book.sheet_names\n",
    "bl_sheet_df = bl_book.parse(bl_sheet_name[0],skiprows=3,skipfooter=5)\n",
    "bl_sheet_df=bl_sheet_df[bl_sheet_df[bl_sheet_df.columns[1]].str.contains(\"\",na=False)] # id exists\n",
    "#print(\"Sheet name:\", bl_sheet_name[0])\n",
    "bl_sheet_df=bl_sheet_df.loc[:,[\"ImPACT_ID\",\"Arm_Type_01\",\"Age_01\",\"Gender_12_01\",\"MRI_Imaging_Date_01\",\\\n",
    "                               \"FABHQ_01\",\"FABHQ_DMN_01\",\"FABHQ_CEN_01\",\"FABHQ_SN_01\",\\\n",
    "                               \"GMBHQ_01\",\"GMBHQ_DMN_01\",\"GMBHQ_CEN_01\",\"GMBHQ_SN_01\",\\\n",
    "                               \"SPA_Unrelated_SUM_01\",\"TMT_PartA_01_01\",\"TMT_PartB_01_01\",\\\n",
    "                               \"MMSEJ_SUM_01\",\"CESD_Score_01\"]] # only base lines have MMSE\n",
    "#print(bl_sheet_df.shape)\n",
    "#print(bl_sheet_df.tail(5))\n",
    "\n",
    "# read follow up\n",
    "fu_sheet_name = fu_book.sheet_names\n",
    "fu_sheet_df=fu_book.parse(fu_sheet_name[0],skiprows=3,skipfooter=5)\n",
    "fu_sheet_df=fu_sheet_df[fu_sheet_df[fu_sheet_df.columns[1]].str.contains(\"\",na=False)] # id exists\n",
    "#print(\"Sheet name:\", fu_sheet_name[0])\n",
    "fu_sheet_df=fu_sheet_df.loc[:,[\"ImPACT_ID\",\"Arm_Type_02\",\"Age_02\",\"Gender_12_02\",\"MRI_Imaging_Date_02\",\\\n",
    "                               \"FABHQ_02\",\"FABHQ_DMN_02\",\"FABHQ_CEN_02\",\"FABHQ_SN_02\",\\\n",
    "                               \"GMBHQ_02\",\"GMBHQ_DMN_02\",\"GMBHQ_CEN_02\",\"GMBHQ_SN_02\",\\\n",
    "                               \"SPA_Unrelated_SUM_01\",\"TMT_PartA_01_02\",\"TMT_PartB_01_02\"]] #,\"MMSEJ_SUM_01\",\"CESD_Score_01\"\n",
    "#print(fu_sheet_df.shape)\n",
    "#print(fu_sheet_df.tail(5))\n",
    "\n",
    "# merge baseline and followup\n",
    "bl_fu_sheet_df=pd.merge(bl_sheet_df, fu_sheet_df, left_on=bl_sheet_df.columns[0], right_on=fu_sheet_df.columns[0], how=\"inner\", suffixes = [\"_bl\", \"_fu\"])\n",
    "bl_fu_sheet_df.columns=bl_fu_sheet_df.columns.str.replace(\" \",\"_\")\n",
    "bl_fu_sheet_df.columns=bl_fu_sheet_df.columns.str.replace(\"\\n\",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove IDs whose QC failed\n",
    "bl_fu_sheet_df=bl_fu_sheet_df[~bl_fu_sheet_df[\"ImPACT_ID\"].isin(IDsQCfailed)]\n",
    "\n",
    "# remove IDs depressed\n",
    "#print(\"IDs depressed:\\n\",bl_fu_sheet_df[bl_fu_sheet_df[\"CESD_Score_01\"]>=16][[\"ImPACT_ID\",\"Arm_Type_01\",\"CESD_Score_01\"]])\n",
    "#bl_fu_sheet_df=bl_fu_sheet_df[bl_fu_sheet_df[\"CESD_Score_01\"]<16]\n",
    "\n",
    "# make data frame\n",
    "df_result=bl_fu_sheet_df.loc[:,[\"ImPACT_ID\",\"Arm_Type_01\",\"Age_01\", \"Age_02\"]]\n",
    "df_result[\"fID\"]=\"V_\"+df_result[\"ImPACT_ID\"]+\"_\"+df_result[\"Arm_Type_01\"]\n",
    "\"\"\"\n",
    "df_result[\"SPA_Unrelated_SUM_01\"]=df_result[\"SPA_Unrelated_SUM_01_bl\"]\n",
    "df_result[\"SPA_Unrelated_SUM_02\"]=df_result[\"SPA_Unrelated_SUM_01_fu\"]\n",
    "df_result=df_result.drop(columns=[\"SPA_Unrelated_SUM_01_bl\",\"SPA_Unrelated_SUM_01_fu\"])\n",
    "df_result=df_result.drop(columns=[\"MRI_Imaging_Date_01\", \"MRI_Imaging_Date_02\"])\n",
    "\"\"\"\n",
    "df_result=df_result.set_index(\"fID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make .mat file for FSL TBSS\n",
    "ctrl_grouplist=[\"AR\"]\n",
    "intrv_grouplist=[\"AS\",\"AT\",\"AU\"]\n",
    "#impact_grouplist=[\"AR\",\"AS\",\"AT\",\"AU\"]\n",
    "suffixlist=[\"_01\",\"_02\"]\n",
    "scorelist=[\"SPA_Unrelated_SUM_01_bl\",\"TMT_PartA_01_01\",\"TMT_PartB_01_01\",\"MMSEJ_SUM_01\",\\\n",
    "           \"SPA_Unrelated_SUM_01_fu\",\"TMT_PartA_01_02\",\"TMT_PartB_01_02\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_tbssfile(temp_filenamebody, temp_dfvarname, temp_varname, suffixlist): # subroutine\n",
    "    for sfx in suffixlist:\n",
    "        for ss in intrv_grouplist:\n",
    "            classlist=[ctrl_grouplist[0], ss]\n",
    "\n",
    "            data1=[]\n",
    "            data2=[]\n",
    "            data3=[]\n",
    "            for temp_line in df_result.itertuples():\n",
    "                temp_armtype=temp_line[df_result.columns.get_loc(\"Arm_Type_01\")+1] # 1 for .index\n",
    "                if temp_armtype in classlist:\n",
    "                    str_dfvar=str(temp_line[df_result.columns.get_loc(temp_dfvarname+sfx)+1]) # 1 for .index\n",
    "                    if str_dfvar.replace('.', '').replace('-', '').isnumeric(): # ignore NaN\n",
    "                        if temp_armtype==classlist[0]:\n",
    "                            is_group1=1\n",
    "                        else:\n",
    "                            is_group1=0\n",
    "                        if temp_armtype==classlist[1]:\n",
    "                            is_group2=1\n",
    "                        else:\n",
    "                            is_group2=0\n",
    "                        data1.append(is_group1)\n",
    "                        data2.append(is_group2)\n",
    "                        data3.append(int(str_dfvar))\n",
    "\n",
    "            filename=IMPACT_CWD+temp_filenamebody+ss+sfx+\".mat.txt\"\n",
    "            outfile=open(filename, \"wt\")\n",
    "            \"\"\"\n",
    "            outfile.write(\"/NumWaves \"+str(3)+\"\\n\")\n",
    "            outfile.write(\"/NumPoints \"+str(len(data1))+\"\\n\")\n",
    "            outfile.write(\"/PPheights \"+str(max(max(data1), 0)-min(min(data1), 0))+\" \"+str(max(max(data2), 0)-min(min(data2), 0))+\" \"+str(max(max(data3), 0)-min(min(data3), 0))+\"\\n\")\n",
    "            outfile.write(\"/Matrix\"+\"\\n\")\n",
    "            outfile.write(\"\\n\")\n",
    "            \"\"\"\n",
    "            for ii in range(len(data1)):\n",
    "                outfile.write(str(data1[ii])+\" \"+str(data2[ii])+\" \"+str(data3[ii])+\"\\n\")\n",
    "            outfile.close()\n",
    "\n",
    "            filename=IMPACT_CWD+temp_filenamebody+ss+sfx+\".con.txt\"\n",
    "            outfile=open(filename, \"wt\")\n",
    "            \"\"\"\n",
    "            outfile.write(\"/ContrastName1\t\"+classlist[0]+\" > \"+classlist[1]+\"\\n\")\n",
    "            outfile.write(\"/ContrastName2\t\"+classlist[0]+\" < \"+classlist[1]+\"\\n\")\n",
    "            outfile.write(\"/ContrastName3\tpos age corr \"+\"\\n\")\n",
    "            outfile.write(\"/ContrastName4\tneg age corr \"+\"\\n\")\n",
    "            outfile.write(\"/NumWaves\t\"+str(3)+\"\\n\")\n",
    "            outfile.write(\"/NumContrasts\t\"+str(4)+\"\\n\")\n",
    "            #outfile.write(\"/PPheights\t\t\"+str(1.023956e+00)+\" \"+str(1.023956e+00)+\" \"+str(2.820455e+01)+\" \"+str(2.820455e+01)+\"\\n\")\n",
    "            #outfile.write(\"/RequiredEffect\t\t\"+str(0.834)+\" \"+str(0.834)+\" \"+str(1.486)+\" \"+str(1.486)+\"\\n\")\n",
    "            #outfile.write(\"\\n\")\n",
    "            #outfile.write(\"/Matrix\"+\"\\n\")\n",
    "            \"\"\"\n",
    "            outfile.write(str(1)+\" \"+str(-1)+\" \"+str(0)+\"\\n\")\n",
    "            outfile.write(str(-1)+\" \"+str(1)+\" \"+str(0)+\"\\n\")\n",
    "            outfile.write(str(0)+\" \"+str(0)+\" \"+str(1)+\"\\n\")\n",
    "            outfile.write(str(0)+\" \"+str(0)+\" \"+str(-1)+\"\\n\")\n",
    "            outfile.close()\n",
    "    return\n",
    "\n",
    "temp_filenamebody=\"design_g2v1_\"\n",
    "temp_dfvarname=\"Age\"\n",
    "#temp_title=\"group_age_\"\n",
    "temp_varname=\"age\"\n",
    "temp_suffixlist=suffixlist\n",
    "make_tbssfile(temp_filenamebody, temp_dfvarname, temp_varname, temp_suffixlist) # call subroutine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
